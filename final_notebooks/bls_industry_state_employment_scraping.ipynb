{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a8233554-7ef3-469b-be1b-58007119343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9973a-7089-44e0-807b-b6c2b4420038",
   "metadata": {},
   "source": [
    "### Script to automate the extraction and processing of the industry level employment data from local HTML files of each state. These files/ web pages further have links to each industry's data for that state.  \n",
    "\n",
    "### Using BeautifulSoup, the below code first reads each HTML file from a local directory and based on the pattern in the industry URLs, this code specifically searches the specific attributes. It then constructs the full URLs to scrape data from. \n",
    "\n",
    "### Next, the script creates a state-specific folder (based on the HTML filename) in the output directory. Each industry URL then is processed by a function called process_url, which scrapes the relevant data, converts it into a DataFrame, and saves it as a CSV file.\n",
    "\n",
    "### Output is : 51 state specific folders are created. Each folder had 12 industry specific employment data as csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c3b5b70d-211c-4eb2-b4a0-1eccee342bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/96_jts790qb22y30bzlhgh_m0000gn/T/ipykernel_29334/966073048.py:24: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  state_row = soup.find(text='State:')\n",
      "/var/folders/0n/96_jts790qb22y30bzlhgh_m0000gn/T/ipykernel_29334/966073048.py:27: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  industry_row = soup.find(text='Industry:')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_url(url, state_folder):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'id': 'table0', 'class': 'regular-data'})\n",
    "    header_row = table.find('thead').find_all('th')\n",
    "    header = [th.get_text(strip=True) for th in header_row]\n",
    "\n",
    "    data = []\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "        data.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    state_row = soup.find(text='State:')\n",
    "    state = state_row.find_next('td').get_text(strip=True).lower()\n",
    "\n",
    "    industry_row = soup.find(text='Industry:')\n",
    "    industry = industry_row.find_next('td').get_text(strip=True).replace(' ', '_').lower()\n",
    "\n",
    "    csv_file_name = os.path.join(state_folder, f\"{state}_{industry}.csv\")\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "html_files_directory = '/Users/alks/Downloads/DDA13/Python/capstone/data/raw_files/industry_htmls/'\n",
    "\n",
    "output_directory = '/Users/alks/Downloads/DDA13/Python/capstone/data/cleaned_files/industry_state/'\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for html_file in os.listdir(html_files_directory):\n",
    "    if html_file.endswith('.html'):\n",
    "        file_path = os.path.join(html_files_directory, html_file)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "    \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        urls = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "           \n",
    "            if 'timeseries' in href:\n",
    "                if href.startswith('/'):\n",
    "                    full_url = f'https://www.bls.gov{href}'\n",
    "                else:\n",
    "                    full_url = href\n",
    "                urls.append(full_url)\n",
    "\n",
    "        # Keep only URLs from the 5th one onwards and remove duplicates\n",
    "        unique_urls = list(dict.fromkeys(urls[4:]))\n",
    "\n",
    "        state_name = html_file.split('_')[0].lower()\n",
    "        state_folder = os.path.join(output_directory, state_name)\n",
    "        os.makedirs(state_folder, exist_ok=True)\n",
    "\n",
    "        for url in unique_urls:\n",
    "            df = process_url(url, state_folder)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d7de2-27ac-43f6-b3e7-98e1ed069434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
